model:
  type: transformer-encoder
  d_model: 256
  encoders:
    date:
      type: no_op
      input_dim: 1
      output_dim: 1
    brand:
      type: embedding
      output_dim: 64
    amount:
      type: no_op
      input_dim: 1
      output_dim: 1
  concat_layer:
    type: mlp
    input_dim: 66  # 1 + 64 + 1
    n_layers: 2
    hidden_dims: [128, 128]
    output_dim: 256
    activation: ReLU
    final_activation: ReLU
  decoder:
    type: linear
    input_dim: 256
  transformer:
    nhead: 8
    num_encoder_layers: 6
    dropout: 0.1
  max_seq_length: 500
  positional_encoding: sinusoidal
  dropout: 0.1
